{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "LydTAN6-6mHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMB__tnd0HW0"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch torchvision pillow gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, ViTModel\n",
        "\n",
        "class MultiModalSentimentModel(nn.Module):\n",
        "    def __init__(self, num_labels=3):\n",
        "        super(MultiModalSentimentModel, self).__init__()\n",
        "\n",
        "        self.text_encoder = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "\n",
        "        self.image_encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768 + 768, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, text_input, image_input):\n",
        "\n",
        "        text_outputs = self.text_encoder(**text_input)\n",
        "        text_feats = text_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "\n",
        "        image_outputs = self.image_encoder(pixel_values=image_input)\n",
        "        image_feats = image_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "\n",
        "        combined_feats = torch.cat((text_feats, image_feats), dim=1)\n",
        "\n",
        "\n",
        "        logits = self.classifier(combined_feats)\n",
        "        return logits\n",
        "\n",
        "# Initialize model and move to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultiModalSentimentModel().to(device)\n",
        "print(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "id": "4SybkRKI0MP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, ViTImageProcessor\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "def preprocess_data(text, image):\n",
        "\n",
        "    inputs_text = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    inputs_image = feature_extractor(images=image, return_tensors=\"pt\")['pixel_values'].to(device)\n",
        "    return inputs_text, inputs_image"
      ],
      "metadata": {
        "id": "I9v_8IxZ0XJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text, image):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        txt_in, img_in = preprocess_data(text, image)\n",
        "        outputs = model(txt_in, img_in)\n",
        "\n",
        "\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        conf, classes = torch.max(probs, dim=1)\n",
        "\n",
        "        labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "        return {labels[i]: float(probs[0][i]) for i in range(3)}"
      ],
      "metadata": {
        "id": "tNQojJFF05pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=predict_sentiment,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, placeholder=\"Enter text (e.g., 'I love this new phone design!')\"),\n",
        "        gr.Image(type=\"pil\", label=\"Upload an associated image\")\n",
        "    ],\n",
        "    outputs=gr.Label(num_top_classes=3),\n",
        "    title=\"Multi-Modal Sentiment Analyzer\",\n",
        "    description=\"Upload an image and its caption to see the combined sentiment score.\"\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "hXR72M90095A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_to_correct(model, text_list, image_tensors, labels, epochs=10):\n",
        "    model.train()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=5e-5) # AdamW is standard for Transformers\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for i in range(len(text_list)):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "\n",
        "            txt_in, img_in = preprocess_data(text_list[i], image_tensors[i])\n",
        "            target = torch.tensor([labels[i]]).to(device)\n",
        "\n",
        "\n",
        "            outputs = model(txt_in, img_in)\n",
        "            loss = criterion(outputs, target)\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} | Avg Loss: {total_loss/len(text_list):.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2V6qidpB2d7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=predict_sentiment,\n",
        "    inputs=[gr.Textbox(), gr.Image(type=\"pil\")],\n",
        "    outputs=gr.Label(),\n",
        ")\n",
        "\n",
        "demo.launch(share=True, debug=False, inline=True)"
      ],
      "metadata": {
        "id": "4RscFtE_5vlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "history_loss = [0.9, 0.7, 0.5, 0.35, 0.2, 0.15, 0.12, 0.1, 0.08, 0.05]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history_loss, marker='o', color='b', linestyle='-')\n",
        "plt.title('Model Correction: Training Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss (Categorical Cross-Entropy)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7rqyezes4Wkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vm83h2184kXM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}